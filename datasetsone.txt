slip 1

import pandas as pd
import numpy as np
df = pd.read_csv('Student_scores.csv')
data.head(5)
from sklearn.model_selection import train_test_split
from sklearn.metrics

X = df.iloc[:,:-1].values
y = df.iloc[:,:1].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Calculate mean absolute error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error (MAE): {mae}')

# Calculate mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error (MSE): {mse}')

# Calculate root mean squared error (RMSE)
rmse = (mse)**0.5
print(f'Root Mean Squared Error (RMSE): {rmse}')


===================================================================================
slip 2

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generate synthetic dataset
X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=0.60)

# Visualize the synthetic dataset
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title('Synthetic Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Apply k-means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# Get cluster centers and labels
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Visualize the clusters and cluster centers
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroids')
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

===================================================================================

slip 3
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13]).reshape(-1,1)
y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12, 16, 18])

m=LinearRegression().fit(x,y)

print(m.intercept_)
print(m.coef_)
===================================================================================


slip 4
import matplotlib.pylot as plt
import sklearn
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder

# Given dataset
weather = ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy']
temp = ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild']
play = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']

# Encode categorical variables
le_weather = LabelEncoder()
le_temp = LabelEncoder()
le_play = LabelEncoder()

weather_encoded = le_weather.fit_transform(weather)
temp_encoded = le_temp.fit_transform(temp)
play_encoded = le_play.fit_transform(play)

# Create features matrix X
X = list(zip(weather_encoded, temp_encoded))

# Create Gaussian Na√Øve Bayes classifier
clf = GaussianNB()
clf.fit(X, play_encoded)

# Predict class for [0: Overcast, 2: Mild]
new_data = [(le_weather.transform(['Overcast'])[0], le_temp.transform(['Mild'])[0])]
predicted_class = clf.predict(new_data)[0]

# Decode the predicted class
predicted_class_label = le_play.inverse_transform([predicted_class])[0]

# Print the result
print(f"The tuple [0: Overcast, 2: Mild] belongs to the class: {predicted_class_label}")

===================================================================================

slip 5 and 16

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Display the first few rows of the dataset
print(df.head())
features_cols=['pregnancies','Insulin','BMI','Age','Glucose','BP','DiabetesPedigreeFunction']
# Separate features (X) and target variable (y)
X = df[features_cols]
y = df['Outcome']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')


===================================================================================

slip 6 and 20
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram linkage

# Load the dataset
df = pd.read_csv('Customer.csv')  

# Display the first few rows of the dataset
print(df.head(5))

# Assuming you want to cluster based on certain columns, for example, 'Annual Income' and 'Spending Score'
X = df[['Annual Income', 'Spending Score']]

# Perform hierarchical agglomerative clustering
model = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward')
cluster_labels = model.fit_predict(X)

# Plot the dendrogram
linked = linkage(X, 'ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Agglomerative Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Cluster Distance')
plt.show()

# Visualize the clusters (for demonstration, you can replace the 'x' and 'y' with your actual column names)
plt.scatter(X['Annual Income'], X['Spending Score'], c=cluster_labels, cmap='viridis')
plt.title('Hierarchical Agglomerative Clustering')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.show()

===================================================================================

slip 7  and 18

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Given data
x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([7, 14, 15, 18, 19, 21, 26, 23])

# Reshape x to a 2D array, required by scikit-learn
x = x.reshape(-1, 1)

# Create a linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(x, y)

# Get the estimated coefficients
b0 = model.intercept_
b1 = model.coef_[0]

# Make predictions on the same data
y_pred = model.predict(x)

# Analyze the performance of the model
mae = mean_absolute_error(y, y_pred)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y, y_pred)

# Print the estimated coefficients
print(f'Intercept (b0): {b0}')
print(f'Slope (b1): {b1}')

# Print the performance metrics
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'R-squared (R2): {r2}')

# Plot the regression line
plt.scatter(x, y, color='blue', label='Actual Data')
plt.plot(x, y_pred, color='red', linewidth=2, label='Regression Line')
plt.title('Simple Linear Regression')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()


===================================================================================

slip 8
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('CC GENERAL.csv')  

# Display the first few rows of the dataset
print(df.head(5))

# Drop unnecessary columns (e.g., customer ID) for clustering
df = df.drop(['CUST_ID'], axis=1)

# Handling missing values (you may want to customize this based on your dataset)
df = df.fillna(df.mean())

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(df_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.show()

# Based on the elbow graph, choose the optimal number of clusters
k_optimal = 3

# Apply k-means clustering with the optimal number of clusters
kmeans_optimal = KMeans(n_clusters=k_optimal, init='k-means++', max_iter=300, n_init=10, random_state=0)
df['Cluster'] = kmeans_optimal.fit_predict(df_scaled)

# Display the clusters
print(df['Cluster'].value_counts())

# Visualize the clusters (for demonstration, you can customize the columns to plot)
plt.scatter(df['BALANCE'], df['PURCHASES'], c=df['Cluster'], cmap='viridis')
plt.title('K-Means Clustering')
plt.xlabel('Balance')
plt.ylabel('Purchases')
plt.show()

===================================================================================


slip 9

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix

# Load the breast cancer dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM model
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f'Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision * 100:.2f}%')
print(f'Recall: {recall * 100:.2f}%')

# Display classification report and confusion matrix
print('\nClassification Report:')
print(classification_report(y_test, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))



===================================================================================


slip 10
import numpy as np
import pandas as pd
from apyori import apriori
dataset=pd.read_csv('Iris.csv')
dataset.head()
records[]

for i in range(0,150):
records.append([Str(dataset.values[i,j]) for j in range(0,5])
rules=apriori(records,min_support=0.003,min_confidence=0.2,min_lift=3,min_length=2,max_length=2)
results=list(rules)
print(len(results))
print(results[0])
for item in results:
pair=item[0]
items=[x for x in pair]
print("Rules"+ items[0]+ ""+ item[1])
print("Support"+ str(item[1]))
print("Confidence"+ str(item[2] [0] [2]))
print("Lift"+ str(item[2] [0] [3]))


===================================================================================


slip 11
import numpy as np
import matplotlib.pyplot as mtp
import pandas as pd
dataset = pd.read_csv('Wholesalecustomer.csv')
dataset
x = dataset.iloc[:, [3, 4]].values
print(x)
import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(x, method="ward"))
mtp.title("Dendrogrma Plot")
mtp.ylabel("Euclidean Distances")
mtp.xlabel("Customers")
mtp.show()
from sklearn.cluster import AgglomerativeClustering
hc= AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
y_pred= hc.fit_predict(x)
mtp.scatter(x[y_pred == 0, 0], x[y_pred == 0, 1], s = 100, c = 'blue', label = 'Cluster 1')
mtp.scatter(x[y_pred == 1, 0], x[y_pred == 1, 1], s = 100, c = 'green', label = 'Cluster 2')
mtp.scatter(x[y_pred== 2, 0], x[y_pred == 2, 1], s = 100, c = 'red', label = 'Cluster 3')
mtp.scatter(x[y_pred == 3, 0], x[y_pred == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
mtp.scatter(x[y_pred == 4, 0], x[y_pred == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
mtp.title('Clusters of customers')
mtp.xlabel('Milk')
mtp.ylabel('Grocery')
mtp.legend()
mtp.show()


===================================================================================


slip12  and 19

import pandas
from sklearn import linear_model
df = pandas.read_csv("car.csv")
X = df[['Weight', 'Volume']]
y = df['CO2']
regr = linear_model.LinearRegression()
regr.fit(X, y)
#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:
predictedCO2 = regr.predict([[2300, 1300]])
print(predictedCO2)


===================================================================================


slip 13

import pandas as pd

# Load the StudentsPerformance dataset
dataset=pd.read_csv("StudentsPerformance.csv")
 


# Display the shape of the dataset
print("Shape of the dataset:", df.shape)

# Display the top rows of the dataset with their columns
print("\nTop rows of the dataset:")
print(df.head())

===================================================================================


slip 14


===================================================================================

slip 15

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing

# Load the dataset from the provided link

df = pd.read_csv()

# Display the first few rows of the dataset
print("Top rows of the dataset:")
print(df.head())

# Preprocess the data and create feature matrix X and target variable y
le = preprocessing.LabelEncoder()

# Convert categorical variables to numerical
df['Nationality'] = le.fit_transform(df['Nationality'])
df['Show Type'] = le.fit_transform(df['Show Type'])
df['Gender'] = le.fit_transform(df['Gender'])

# Features (age, experience, comedy ranking)
X = df[['Age', 'Experience', 'Ranking']]

# Target variable (Class Label)
y = df['Class Label']

# Create Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X, y)

# Make a prediction for a comedian with age 40, experience 10, and comedy ranking 7
new_comedian = pd.DataFrame({'Age': [40], 'Experience': [10], 'Ranking': [7]})
predicted_class = clf.predict(new_comedian)

# Decode the predicted class
predicted_class_label = le.inverse_transform(predicted_class)[0]

# Print the result
print(f"\nPredicted Class Label for the comedian: {predicted_class_label}")




===================================================================================

slip 17

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Create the Stock_Market dataframe
Stock_Market = {
    'Year': [2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016],
    'Month': [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],
    'Interest_Rate': [2.75, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.25, 2.25, 2.25, 2, 2, 2, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75, 1.75],
    'Unemployment_Rate': [5.3, 5.3, 5.3, 5.3, 5.4, 5.6, 5.5, 5.5, 5.5, 5.6, 5.7, 5.9, 6, 5.9, 5.8, 6.1, 6.2, 6.1, 6.1, 6.1, 5.9, 6.2, 6.2, 6.1],
    'Stock_Index_Price': [1464, 1394, 1357, 1293, 1256, 1254, 1234, 1195, 1159, 1167, 1130, 1075, 1047, 965, 943, 958, 971, 949, 884, 866, 876, 822, 704, 719]
}

df = pd.DataFrame(Stock_Market)

# Display the shape of the dataset
print("Shape of the dataset:", df.shape)

# Display the top rows of the dataset with their columns
print("\nTop rows of the dataset:")
print(df.head())

# Draw a graph of stock market price versus interest rate
sns.scatterplot(x='Interest_Rate', y='Stock_Index_Price', data=df)
plt.title('Stock Market Price vs. Interest Rate')
plt.xlabel('Interest Rate')
plt.ylabel('Stock Index Price')
plt.show()

# Perform multiple linear regression
X = df[['Interest_Rate']]
y = df['Stock_Index_Price']

model = LinearRegression()
model.fit(X, y)

# Plot the regression line
sns.scatterplot(x='Interest_Rate', y='Stock_Index_Price', data=df)
plt.plot(X, model.predict(X), color='red', linewidth=2)
plt.title('Stock Market Price vs. Interest Rate with Regression Line')
plt.xlabel('Interest Rate')
plt.ylabel('Stock Index Price')
plt.show()


===================================================================================




